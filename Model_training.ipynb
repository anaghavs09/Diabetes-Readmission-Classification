{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3zAMhLoYi50d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9vQKIaxDi7uj"
   },
   "outputs": [],
   "source": [
    "\n",
    "ids_file = \"IDS_mapping.csv\"\n",
    "data_file = \"diabetic_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HHZUG-mtibVp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   encounter_id  patient_nbr             race  gender      age weight  \\\n",
      "0       2278392      8222157        Caucasian  Female   [0-10)      ?   \n",
      "1        149190     55629189        Caucasian  Female  [10-20)      ?   \n",
      "2         64410     86047875  AfricanAmerican  Female  [20-30)      ?   \n",
      "3        500364     82442376        Caucasian    Male  [30-40)      ?   \n",
      "4         16680     42519267        Caucasian    Male  [40-50)      ?   \n",
      "\n",
      "   admission_type_id  discharge_disposition_id  admission_source_id  \\\n",
      "0                  6                        25                    1   \n",
      "1                  1                         1                    7   \n",
      "2                  1                         1                    7   \n",
      "3                  1                         1                    7   \n",
      "4                  1                         1                    7   \n",
      "\n",
      "   time_in_hospital  ... citoglipton insulin  glyburide-metformin  \\\n",
      "0                 1  ...          No      No                   No   \n",
      "1                 3  ...          No      Up                   No   \n",
      "2                 2  ...          No      No                   No   \n",
      "3                 2  ...          No      Up                   No   \n",
      "4                 1  ...          No  Steady                   No   \n",
      "\n",
      "   glipizide-metformin  glimepiride-pioglitazone  metformin-rosiglitazone  \\\n",
      "0                   No                        No                       No   \n",
      "1                   No                        No                       No   \n",
      "2                   No                        No                       No   \n",
      "3                   No                        No                       No   \n",
      "4                   No                        No                       No   \n",
      "\n",
      "   metformin-pioglitazone  change diabetesMed readmitted  \n",
      "0                      No      No          No         NO  \n",
      "1                      No      Ch         Yes        >30  \n",
      "2                      No      No         Yes         NO  \n",
      "3                      No      Ch         Yes         NO  \n",
      "4                      No      Ch         Yes         NO  \n",
      "\n",
      "[5 rows x 50 columns]\n",
      "IDS DataFrame shape: (67, 2)\n",
      "Main DataFrame shape: (101766, 50)\n",
      "Initial DataFrame shape: (101766, 50)\n",
      "After cleaning: (100241, 42)\n",
      "Low-variance columns: ['examide', 'citoglipton']\n",
      "Before aggregation, df type: <class 'pandas.core.frame.DataFrame'>, shape: (100241, 40)\n",
      "After aggregation, df type: <class 'pandas.core.frame.DataFrame'>, shape: (100241, 38)\n",
      "Updated drug keys: ['metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', 'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide', 'insulin', 'glyburide-metformin', 'glipizide-metformin', 'glimepiride-pioglitazone', 'metformin-rosiglitazone', 'metformin-pioglitazone', 'change', 'diabetesMed']\n",
      "Preprocessed data saved to: data/preprocessed_data.csv\n"
     ]
    }
   ],
   "source": [
    "def load_data(ids_file, data_file):\n",
    "\n",
    "    if not os.path.exists(ids_file) or not os.path.exists(data_file):\n",
    "        raise FileNotFoundError(f\"One or more input files not found: {ids_file}, {data_file}\")\n",
    "\n",
    "    ids_df = pd.read_csv(ids_file)\n",
    "    df = pd.read_csv(data_file)\n",
    "    print(df.head())  # Print the first few rows to check if the file is loaded correctly\n",
    "    print(f\"IDS DataFrame shape: {ids_df.shape}\")\n",
    "    print(f\"Main DataFrame shape: {df.shape}\")\n",
    "    return ids_df, df\n",
    "\n",
    "\n",
    "def split_ids_mapping(ids_df):\n",
    "    admission_type_id_df = ids_df.iloc[:8]\n",
    "    discharge_disposition_id_df = ids_df.iloc[10:40].rename(columns={'admission_type_id': 'discharge_disposition_id'})\n",
    "    admission_source_id_df = ids_df.iloc[42:].rename(columns={'admission_type_id': 'admission_source_id'})\n",
    "    return admission_type_id_df, discharge_disposition_id_df, admission_source_id_df\n",
    "\n",
    "\n",
    "def clean_data(df):\n",
    "    # Drop high null value columns\n",
    "    df = df.drop(df[df['gender'] == 'Unknown/Invalid'].index)\n",
    "    df.drop(['max_glu_serum', 'A1Cresult', 'weight', 'payer_code', 'medical_specialty', 'encounter_id', 'race', 'patient_nbr'], axis=1, inplace=True)\n",
    "    df.replace('?', np.nan, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Check if the DataFrame is empty\n",
    "    if df.empty:\n",
    "        print(\"DataFrame is empty after cleaning. Please check the data source.\")\n",
    "        return None  # Return None to indicate an issue\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_low_variance_columns(df, threshold=0.999999999):\n",
    "    \"\"\"\n",
    "    Drops low-variance categorical columns from the DataFrame and returns the modified DataFrame\n",
    "    along with the list of dropped columns.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        threshold (float): The maximum proportion of the most common category value\n",
    "                           to consider as low variance.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with low-variance columns removed.\n",
    "        list: List of dropped column names.\n",
    "    \"\"\"\n",
    "    # Identify categorical columns\n",
    "    categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    # Find low-variance columns\n",
    "    low_variance_cols = [\n",
    "        col for col in categorical_columns\n",
    "        if df[col].value_counts(normalize=True).max() > threshold\n",
    "    ]\n",
    "\n",
    "    print(f\"Low-variance columns: {low_variance_cols}\")\n",
    "\n",
    "    # Drop low-variance columns from the DataFrame\n",
    "    df = df.drop(low_variance_cols, axis=1)\n",
    "\n",
    "    return df, low_variance_cols\n",
    "\n",
    "\n",
    "\n",
    "def aggregate_service_utilization(df):\n",
    "    required_columns = ['number_outpatient', 'number_emergency', 'number_inpatient']\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise KeyError(f\"Missing required columns for aggregation: {missing_columns}\")\n",
    "\n",
    "    df['service_utilization'] = df['number_outpatient'] + df['number_emergency'] + df['number_inpatient']\n",
    "    df.drop(['number_outpatient', 'number_emergency', 'number_inpatient'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def encode_drug_changes(df, keys):\n",
    "    for col in keys:\n",
    "        colname = f\"{col}temp\"\n",
    "        df[colname] = df[col].apply(lambda x: 0 if (x == 'No' or x == 'Steady') else 1)\n",
    "\n",
    "    df['numchange'] = 0\n",
    "    for col in keys:\n",
    "        colname = f\"{col}temp\"\n",
    "        df['numchange'] += df[colname]\n",
    "        df.drop(colname, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def map_admission_and_discharge(df):\n",
    "    # Map `admission_type_id`\n",
    "    admission_type_mapping = {2: 1, 7: 1, 6: 5, 8: 5}\n",
    "    df['admission_type_id'] = df['admission_type_id'].replace(admission_type_mapping)\n",
    "\n",
    "    # Map `discharge_disposition_id`\n",
    "    discharge_mapping = {\n",
    "        6: 1, 8: 1, 9: 1, 13: 1,\n",
    "        3: 2, 4: 2, 5: 2, 14: 2, 22: 2, 23: 2, 24: 2,\n",
    "        12: 10, 15: 10, 16: 10, 17: 10,\n",
    "        25: 18, 26: 18\n",
    "    }\n",
    "    df['discharge_disposition_id'] = df['discharge_disposition_id'].replace(discharge_mapping)\n",
    "    return df\n",
    "\n",
    "def map_admission_source(df):\n",
    "    admission_source_mapping = {\n",
    "        2: 1, 3: 1,\n",
    "        5: 4, 6: 4, 10: 4, 22: 4, 25: 4,\n",
    "        15: 9, 17: 9, 20: 9, 21: 9,\n",
    "        13: 11, 14: 11\n",
    "    }\n",
    "    df['admission_source_id'] = df['admission_source_id'].replace(admission_source_mapping)\n",
    "    return df\n",
    "\n",
    "def encode_categorical_columns(df):\n",
    "    df['change'] = df['change'].replace({'Ch': 1, 'No': 0})\n",
    "    df['gender'] = df['gender'].replace({'Male': 1, 'Female': 0})\n",
    "    # df = df.drop(df[df['gender'] == 'Unknown/Invalid'].index)\n",
    "    df['diabetesMed'] = df['diabetesMed'].replace({'Yes': 1, 'No': 0})\n",
    "    return df\n",
    "\n",
    "def encode_drugs(df, keys):\n",
    "    for col in keys:\n",
    "        df[col] = df[col].replace({'No': 0, 'Steady': 1, 'Up': 1, 'Down': 1})\n",
    "    return df\n",
    "\n",
    "def encode_age(df):\n",
    "    for i in range(0, 10):\n",
    "        df['age'] = df['age'].replace(f'[{10*i}-{10*(i+1)})', i+1)\n",
    "    return df\n",
    "\n",
    "def preprocess_diagnosis(df):\n",
    "    for diag in ['diag_1', 'diag_2', 'diag_3']:\n",
    "        level1_col = f'level1_{diag}'\n",
    "        level2_col = f'level2_{diag}'\n",
    "        df[level1_col] = df[diag].replace(r'[VE].*', 0, regex=True).astype(float)\n",
    "        df[level2_col] = df[level1_col]\n",
    "    df.drop(['diag_1', 'diag_2', 'diag_3'], axis=1, inplace = True)\n",
    "    return df\n",
    "\n",
    "def main_preprocess_fn():\n",
    "\n",
    "    # Load data\n",
    "    ids_df, df = load_data(ids_file, data_file)\n",
    "\n",
    "    # Split mappings\n",
    "    _, _, _ = split_ids_mapping(ids_df)\n",
    "\n",
    "    # Clean and preprocess data\n",
    "    print(f\"Initial DataFrame shape: {df.shape}\")\n",
    "    df = clean_data(df)\n",
    "    print(f\"After cleaning: {df.shape}\")\n",
    "\n",
    "    df, dropped_columns = drop_low_variance_columns(df)\n",
    "    print(f\"Before aggregation, df type: {type(df)}, shape: {df.shape if df is not None else 'N/A'}\")\n",
    "    df = aggregate_service_utilization(df)\n",
    "    print(f\"After aggregation, df type: {type(df)}, shape: {df.shape if df is not None else 'N/A'}\")\n",
    "    # # Drug change encoding\n",
    "    # keys = ['metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride',\n",
    "    #         'glipizide', 'glyburide', 'pioglitazone', 'rosiglitazone', 'acarbose',\n",
    "    #         'miglitol', 'insulin', 'glyburide-metformin', 'tolazamide',\n",
    "    #         'metformin-pioglitazone', 'glimepiride-pioglitazone', 'glipizide-metformin',\n",
    "    #         'troglitazone', 'tolbutamide', 'acetohexamide']\n",
    "\n",
    "    # Update keys based on remaining columns\n",
    "    all_drug_columns = ['metformin', 'repaglinide', 'nateglinide', 'chlorpropamide',\n",
    "       'glimepiride', 'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide',\n",
    "       'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone',\n",
    "       'tolazamide', 'examide', 'citoglipton', 'insulin',\n",
    "       'glyburide-metformin', 'glipizide-metformin',\n",
    "       'glimepiride-pioglitazone', 'metformin-rosiglitazone',\n",
    "       'metformin-pioglitazone', 'change', 'diabetesMed']\n",
    "    keys = [col for col in all_drug_columns if col in df.columns]\n",
    "    print(f\"Updated drug keys: {keys}\")\n",
    "\n",
    "    df = encode_drug_changes(df, keys)\n",
    "\n",
    "    # Map IDs\n",
    "    df = map_admission_and_discharge(df)\n",
    "    df = map_admission_source(df)\n",
    "\n",
    "    # Encode categorical columns\n",
    "    df = encode_categorical_columns(df)\n",
    "    df = encode_drugs(df, keys)\n",
    "    df = encode_age(df)\n",
    "\n",
    "    # Process diagnosis columns\n",
    "    df = preprocess_diagnosis(df)\n",
    "\n",
    "    # Save preprocessed data to 'data' folder\n",
    "    output_path = os.path.join(\"data\", \"preprocessed_data.csv\")\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)  # Create the folder if it doesn't exist\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Preprocessed data saved to: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_preprocess_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "YMjD--B7ibVs"
   },
   "outputs": [],
   "source": [
    "# Use the downloaded preprocessed_data.csv here\n",
    "df_clean = pd.read_csv('data/preprocessed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "izH1d10PibVs"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>admission_type_id</th>\n",
       "      <th>discharge_disposition_id</th>\n",
       "      <th>admission_source_id</th>\n",
       "      <th>time_in_hospital</th>\n",
       "      <th>num_lab_procedures</th>\n",
       "      <th>num_procedures</th>\n",
       "      <th>num_medications</th>\n",
       "      <th>number_diagnoses</th>\n",
       "      <th>...</th>\n",
       "      <th>diabetesMed</th>\n",
       "      <th>readmitted</th>\n",
       "      <th>service_utilization</th>\n",
       "      <th>numchange</th>\n",
       "      <th>level1_diag_1</th>\n",
       "      <th>level2_diag_1</th>\n",
       "      <th>level1_diag_2</th>\n",
       "      <th>level2_diag_2</th>\n",
       "      <th>level1_diag_3</th>\n",
       "      <th>level2_diag_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>&gt;30</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>276.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>250.01</td>\n",
       "      <td>250.01</td>\n",
       "      <td>255.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>NO</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>648.0</td>\n",
       "      <td>648.0</td>\n",
       "      <td>250.00</td>\n",
       "      <td>250.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>250.43</td>\n",
       "      <td>250.43</td>\n",
       "      <td>403.0</td>\n",
       "      <td>403.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>NO</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>197.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>157.00</td>\n",
       "      <td>157.00</td>\n",
       "      <td>250.0</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>&gt;30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>414.0</td>\n",
       "      <td>414.0</td>\n",
       "      <td>411.00</td>\n",
       "      <td>411.00</td>\n",
       "      <td>250.0</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender  age  admission_type_id  discharge_disposition_id  \\\n",
       "0       0    2                  1                         1   \n",
       "1       0    3                  1                         1   \n",
       "2       1    4                  1                         1   \n",
       "3       1    5                  1                         1   \n",
       "4       1    6                  1                         1   \n",
       "\n",
       "   admission_source_id  time_in_hospital  num_lab_procedures  num_procedures  \\\n",
       "0                    7                 3                  59               0   \n",
       "1                    7                 2                  11               5   \n",
       "2                    7                 2                  44               1   \n",
       "3                    7                 1                  51               0   \n",
       "4                    1                 3                  31               6   \n",
       "\n",
       "   num_medications  number_diagnoses  ...  diabetesMed  readmitted  \\\n",
       "0               18                 9  ...            1         >30   \n",
       "1               13                 6  ...            1          NO   \n",
       "2               16                 7  ...            1          NO   \n",
       "3                8                 5  ...            1          NO   \n",
       "4               16                 9  ...            1         >30   \n",
       "\n",
       "   service_utilization  numchange  level1_diag_1  level2_diag_1  \\\n",
       "0                    0          3          276.0          276.0   \n",
       "1                    3          1          648.0          648.0   \n",
       "2                    0          3            8.0            8.0   \n",
       "3                    0          2          197.0          197.0   \n",
       "4                    0          1          414.0          414.0   \n",
       "\n",
       "   level1_diag_2  level2_diag_2  level1_diag_3  level2_diag_3  \n",
       "0         250.01         250.01          255.0          255.0  \n",
       "1         250.00         250.00            0.0            0.0  \n",
       "2         250.43         250.43          403.0          403.0  \n",
       "3         157.00         157.00          250.0          250.0  \n",
       "4         411.00         411.00          250.0          250.0  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AplybhnJibVt"
   },
   "source": [
    "#### Test - Train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Sdji5Bh2ibVt"
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X = df_clean.drop('readmitted', axis=1)\n",
    "y = df_clean['readmitted']\n",
    "# Map target labels to integers\n",
    "label_mapping = {'<30': 1, 'NO': 0, '>30': -1}\n",
    "y = y.map(label_mapping)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Apply SMOTE only to the training data\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Ensure the data is numeric (if needed)\n",
    "X_resampled = pd.DataFrame(X_resampled).apply(pd.to_numeric, errors='coerce')\n",
    "X_resampled = np.array(X_resampled, dtype=float)\n",
    "y_resampled = np.array(y_resampled, dtype=int)\n",
    "\n",
    "# Check for NaNs\n",
    "if np.isnan(X_resampled).any():\n",
    "    raise ValueError(\"SMOTE-generated data contains NaN values.\")\n",
    "\n",
    "# Compute mean and standard deviation for each feature in the training set\n",
    "mean = np.mean(X_resampled, axis=0)\n",
    "std = np.std(X_resampled, axis=0)\n",
    "\n",
    "# Avoid division by zero in case of constant features\n",
    "std[std == 0] = 1\n",
    "\n",
    "# Scale the training set\n",
    "X_train_scaled = (X_resampled - mean) / std\n",
    "X_train_scaled = np.array(X_train_scaled, dtype=float)\n",
    "\n",
    "# Scale the test set using the same mean and std as the training set\n",
    "X_test_scaled = (X_test - mean) / std\n",
    "\n",
    "# Ensure the scaled test data is numeric\n",
    "X_test_scaled = np.array(X_test_scaled, dtype=float)\n",
    "\n",
    "X_train_scaled_np = X_train_scaled.astype(np.float64)\n",
    "y_resampled_np = y_resampled.astype(np.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5602HOhbibVu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in original y_train:\n",
      "readmitted\n",
      " 0    43054\n",
      "-1    28138\n",
      " 1     9000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution in resampled y_train:\n",
      " 0    43054\n",
      "-1    43054\n",
      " 1    43054\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Output to check\n",
    "print(\"Class distribution in original y_train:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "print(\"\\nClass distribution in resampled y_train:\")\n",
    "print(pd.Series(y_resampled).value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qa8plCPvibVu"
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KCNGdmtzibVu"
   },
   "outputs": [],
   "source": [
    "# Logistic Regression Implementation\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.classes_ = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        z = np.clip(z, -500, 500)  # Clip values to avoid overflow\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)  # Store unique class labels\n",
    "        if len(self.classes_) > 2:\n",
    "            raise ValueError(\"This implementation supports only binary classification. Use one-vs-all for multiclass.\")\n",
    "\n",
    "        m, n = X.shape\n",
    "        self.weights = np.zeros(n)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.iterations):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            predictions = self.sigmoid(linear_model)\n",
    "            error = predictions - y\n",
    "            self.weights -= (self.learning_rate / m) * np.dot(X.T, error)\n",
    "            self.bias -= (self.learning_rate / m) * np.sum(error)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        probabilities = self.sigmoid(linear_model)\n",
    "        return np.vstack([1 - probabilities, probabilities]).T  # Probability for each class\n",
    "\n",
    "    def predict(self, X):\n",
    "        probabilities = self.predict_proba(X)\n",
    "        return np.argmax(probabilities, axis=1)  # Class with the highest probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "4nA7mVmQibVu"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def one_vs_all_custom(model_class, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Custom implementation of One-vs-All for multi-class classification with evaluation metrics.\n",
    "\n",
    "    Parameters:\n",
    "        model_class: A class that implements fit and predict_proba methods.\n",
    "                     A new instance will be created for each class.\n",
    "        X_train: Training features (numpy array or similar structure).\n",
    "        y_train: Training labels (numpy array, one-dimensional).\n",
    "        X_test: Test features for prediction (numpy array or similar structure).\n",
    "        y_test: True labels for X_test (numpy array, one-dimensional).\n",
    "\n",
    "    Returns:\n",
    "        predictions: Array of predicted class labels for X_test.\n",
    "        models: List of trained binary classifiers, one for each class.\n",
    "    \"\"\"\n",
    "    classes = np.unique(y_train)  # Get all unique class labels\n",
    "    print(\"Classes unique values:\", classes)\n",
    "    models = []  # To store trained models\n",
    "    scores = np.zeros((X_test.shape[0], len(classes)))  # To store scores for each class\n",
    "\n",
    "    # Train one model for each class\n",
    "    for i, c in enumerate(classes):\n",
    "        # Create binary labels for the current class (1 for current class, 0 otherwise)\n",
    "        binary_y_train = (y_train == c).astype(int)\n",
    "\n",
    "        # Train a new instance of the model for the current class\n",
    "        model = model_class()\n",
    "        model.fit(X_train, binary_y_train)\n",
    "        models.append(model)\n",
    "\n",
    "        # Get probabilities for the current class\n",
    "        scores[:, i] = model.predict_proba(X_test)[:, 1]  # Take probability for class 1\n",
    "\n",
    "\n",
    "    # Final predictions are the classes with the highest score\n",
    "    predictions = np.argmax(scores, axis=1)\n",
    "\n",
    "    # Calculate evaluation metrics for each class\n",
    "    print(\"Evaluation Metrics (per class):\")\n",
    "    for i, c in enumerate(classes):\n",
    "        binary_y_test = (y_test == c).astype(int)  # Create binary labels for y_test\n",
    "        binary_predictions = (predictions == i).astype(int)  # Predicted labels for class i\n",
    "\n",
    "        acc = accuracy_score(binary_y_test, binary_predictions)\n",
    "        prec = precision_score(binary_y_test, binary_predictions, zero_division=0)\n",
    "        rec = recall_score(binary_y_test, binary_predictions, zero_division=0)\n",
    "        f1 = f1_score(binary_y_test, binary_predictions, zero_division=0)\n",
    "\n",
    "        print(f\"Class {c}: Accuracy={acc:.2f}, Precision={prec:.2f}, Recall={rec:.2f}, F1={f1:.2f}\")\n",
    "\n",
    "    return predictions, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "094bBWAribVu"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def evaluate_all_classes(y_test, predictions):\n",
    "    # Compute overall accuracy\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "    # Compute precision, recall, F1 for all classes\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(y_test, predictions, average=None)\n",
    "\n",
    "    # Display the metrics for each class\n",
    "    print(f\"Overall Accuracy: {accuracy:.2f}\")\n",
    "    print(\"\\nEvaluation Metrics (per class):\")\n",
    "    for i, class_label in enumerate(np.unique(y_test)):\n",
    "        print(f\"Class {class_label}: Precision={precision[i]:.2f}, Recall={recall[i]:.2f}, F1={f1[i]:.2f}, Support={support[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZ9PiVbRibVv"
   },
   "source": [
    "#### 1. Logistics Regression training - imbalanced data\n",
    "\n",
    "* The train dataset is not balanced with equal weight for all the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "tvCaD1TDibVv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes unique values: [-1.  0.  1.]\n",
      "Evaluation Metrics (per class):\n",
      "Class -1.0: Accuracy=0.63, Precision=0.45, Recall=0.21, F1=0.28\n",
      "Class 0.0: Accuracy=0.57, Precision=0.59, Recall=0.60, F1=0.60\n",
      "Class 1.0: Accuracy=0.67, Precision=0.13, Recall=0.35, F1=0.19\n",
      "Overall Accuracy: 0.43\n",
      "\n",
      "Evaluation Metrics (per class):\n",
      "Class -1: Precision=0.45, Recall=0.21, F1=0.28, Support=7035\n",
      "Class 0: Precision=0.59, Recall=0.60, F1=0.60, Support=10764\n",
      "Class 1: Precision=0.13, Recall=0.35, F1=0.19, Support=2250\n"
     ]
    }
   ],
   "source": [
    "# Use one-vs-all with Logistic Regression\n",
    "predictions, models = one_vs_all_custom(LogisticRegression, X_train_scaled_np, y_resampled_np, X_test_scaled, y_test)\n",
    "\n",
    "# Convert predictions to the same format as y_test (strings)\n",
    "unique_classes = np.unique(y_test)  # Get unique class labels in y_test\n",
    "predictions_str = [unique_classes[pred] for pred in predictions]  # Map the integer predictions to the corresponding class labels\n",
    "\n",
    "# Now evaluate the classes\n",
    "evaluate_all_classes(y_test, predictions_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "uIZyE8BxibVv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-1,  0,  1]), array([28138, 43054,  9000]))\n",
      "(array([-1,  0,  1]), array([ 7035, 10764,  2250]))\n",
      "(array([0, 1, 2]), array([ 3252, 10885,  5912]))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_train, return_counts=True))\n",
    "print(np.unique(y_test, return_counts=True))\n",
    "print(np.unique(predictions, return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjiKVKVUibVv"
   },
   "source": [
    "#### 2. Logistics Regression training - resampled balanced data\n",
    "\n",
    "* The train dataset is balanced with equal weight for all the classes using SMOTE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "519yNm0kibVv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes unique values: [-1.  0.  1.]\n",
      "Evaluation Metrics (per class):\n",
      "Class -1.0: Accuracy=0.63, Precision=0.45, Recall=0.21, F1=0.28\n",
      "Class 0.0: Accuracy=0.57, Precision=0.59, Recall=0.60, F1=0.60\n",
      "Class 1.0: Accuracy=0.67, Precision=0.13, Recall=0.35, F1=0.19\n",
      "Overall Accuracy: 0.43\n",
      "\n",
      "Evaluation Metrics (per class):\n",
      "Class -1: Precision=0.45, Recall=0.21, F1=0.28, Support=7035\n",
      "Class 0: Precision=0.59, Recall=0.60, F1=0.60, Support=10764\n",
      "Class 1: Precision=0.13, Recall=0.35, F1=0.19, Support=2250\n"
     ]
    }
   ],
   "source": [
    "# Use one-vs-all with Logistic Regression\n",
    "predictions, models = one_vs_all_custom(LogisticRegression, X_train_scaled_np, y_resampled_np, X_test_scaled, y_test)\n",
    "\n",
    "# Convert predictions to the same format as y_test (strings)\n",
    "unique_classes = np.unique(y_test)  # Get unique class labels in y_test\n",
    "predictions_str = [unique_classes[pred] for pred in predictions]  # Map the integer predictions to the corresponding class labels\n",
    "\n",
    "# Now evaluate the classes\n",
    "evaluate_all_classes(y_test, predictions_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "45h4E26dibVv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-1,  0,  1]), array([28138, 43054,  9000]))\n",
      "(array([-1,  0,  1]), array([ 7035, 10764,  2250]))\n",
      "(array([0, 1, 2]), array([ 3252, 10885,  5912]))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_train, return_counts=True))\n",
    "print(np.unique(y_test, return_counts=True))\n",
    "print(np.unique(predictions, return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pg_kHXGqibVv"
   },
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "p-kXePevibVv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tqdm import tqdm\n",
    "# from src.one_vs_all_method import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "class HardMarginSVM:\n",
    "    def __init__(self, learning_rate: float, n_iter: int = 1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iter = n_iter\n",
    "        self.w = None  # Weights, to be initialized during training\n",
    "        self.b = 0  # Bias term\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the SVM model using gradient descent.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Input features (numpy array of shape [n_samples, n_features]).\n",
    "        - y: Target labels (numpy array of shape [n_samples]).\n",
    "        \"\"\"\n",
    "        # Convert to NumPy arrays if not already\n",
    "        if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n",
    "            X = X.to_numpy()\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.to_numpy()\n",
    "\n",
    "        # Ensure numeric types\n",
    "        X = np.array(X, dtype=np.float64)\n",
    "        y = np.array(y, dtype=np.int64)\n",
    "        # print('#################################################')\n",
    "        # print(f\"X_resampled sample: {X[:5]}\")\n",
    "        # print(f\"y_resampled sample: {y[:5]}\")\n",
    "        # print('#################################################')\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        self.w = np.zeros(n_features)  # Initialize weights\n",
    "        self.b = 0  # Initialize bias\n",
    "\n",
    "        # print(f\"x_i dtype: {x_i.dtype}, x_i: {x_i}\")\n",
    "        print(f\"self.w dtype: {self.w.dtype}, self.w: {self.w}\")\n",
    "        print(f\"self.b dtype: {type(self.b)}, self.b: {self.b}\")\n",
    "\n",
    "\n",
    "        # Initialize the progress bar\n",
    "        for iteration in tqdm(range(self.n_iter), desc=\"Training Progress\", unit=\"iter\"):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
    "                if condition:\n",
    "                    self.w -= self.learning_rate * (2 * 1 / n_samples * self.w)\n",
    "                else:\n",
    "                    self.w -= self.learning_rate * (2 * 1 / n_samples * self.w - np.dot(x_i, y[idx]))\n",
    "                    self.b -= self.learning_rate * y[idx]\n",
    "\n",
    "            # Optionally, print the progress at specific intervals\n",
    "            if iteration % 100 == 0:\n",
    "                print(f\"Iteration {iteration}/{self.n_iter}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Decision function to classify based on the sign of the result\n",
    "        pred = np.dot(X, self.w) + self.b\n",
    "        return np.sign(pred)\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        # Compute the distance from the hyperplane\n",
    "        return np.dot(X, self.w) + self.b\n",
    "\n",
    "    def evaluate(self, y_test, y_pred):\n",
    "        # Calculate accuracy, precision, recall, and F1 score\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        # Print the metrics\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        print(\"Precision:\", precision)\n",
    "        print(\"Recall:\", recall)\n",
    "        print(\"F1 Score:\", f1)\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(conf_matrix)\n",
    "\n",
    "        return accuracy, precision, recall, f1, conf_matrix\n",
    "\n",
    "\n",
    "\n",
    "    def run_model_svm_hard_margin(self, X_train, X_test, y_train, y_test):\n",
    "        # Fit the model\n",
    "        self.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on test data\n",
    "        y_pred = self.predict(X_test)\n",
    "\n",
    "        # Evaluate model performance and return metrics\n",
    "        return self.evaluate(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jcbi9-DTibVw",
    "outputId": "cdb33f92-9c4b-4d05-ee83-c55788072975"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the model for target class 1\n",
      "self.w dtype: float64, self.w: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "self.b dtype: <class 'int'>, self.b: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|                      | 1/500 [00:00<04:18,  1.93iter/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  20%|████                | 101/500 [00:51<03:25,  1.94iter/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  40%|████████            | 201/500 [01:42<02:34,  1.94iter/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 200/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  60%|████████████        | 301/500 [02:47<04:11,  1.26s/iter]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 300/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  80%|████████████████    | 401/500 [03:39<00:49,  1.99iter/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 400/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|████████████████████| 500/500 [04:28<00:00,  1.86iter/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the model for target class 1\n",
      "Predicting the scores for target class 1\n",
      "Predicting the scores for target class done 1\n",
      "Fitting the model for target class 0\n",
      "self.w dtype: float64, self.w: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "self.b dtype: <class 'int'>, self.b: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|                      | 1/500 [00:00<04:15,  1.96iter/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  20%|████                | 101/500 [00:51<03:25,  1.94iter/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  40%|████████            | 201/500 [01:42<02:31,  1.97iter/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 200/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  60%|████████████        | 301/500 [02:32<01:40,  1.98iter/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 300/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  80%|████████████████    | 401/500 [03:23<00:50,  1.98iter/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 400/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|████████████████████| 500/500 [04:13<00:00,  1.97iter/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the model for target class 0\n",
      "Predicting the scores for target class 0\n",
      "Predicting the scores for target class done 0\n",
      "Fitting the model for target class -1\n",
      "self.w dtype: float64, self.w: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "self.b dtype: <class 'int'>, self.b: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|                      | 1/500 [00:00<04:18,  1.93iter/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  20%|████                | 101/500 [00:52<03:22,  1.97iter/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  40%|████████            | 201/500 [01:43<02:31,  1.97iter/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 200/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  60%|████████████        | 301/500 [02:37<01:44,  1.90iter/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 300/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  80%|████████████████    | 401/500 [03:28<00:49,  1.98iter/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 400/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|████████████████████| 500/500 [04:18<00:00,  1.93iter/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the model for target class -1\n",
      "Predicting the scores for target class -1\n",
      "Predicting the scores for target class done -1\n",
      "Final prediction of classes DONE!!!\n",
      "y_test type: <class 'pandas.core.series.Series'>, y_test unique values: [-1  0  1]\n",
      "final_predictions type: <class 'numpy.ndarray'>, final_predictions unique values: [0]\n",
      "************************************************************************************\n",
      "final_predictions: [0 0 0 ... 0 0 0]\n",
      "************************************************************************************\n",
      "y_test unique values and counts: readmitted\n",
      " 0    10764\n",
      "-1     7035\n",
      " 1     2250\n",
      "Name: count, dtype: int64\n",
      "************************************************************************************\n",
      "final_predictions unique values and counts: (array([0]), array([20049]))\n",
      "************************************************************************************\n",
      "Evaluating the model\n",
      "accuracy 0.5368846326500075\n",
      "Accuracy: 0.5368846326500075\n",
      "Precision: 0.2882451087757335\n",
      "Recall: 0.5368846326500075\n",
      "F1 Score: 0.375103117894699\n",
      "Confusion Matrix:\n",
      "[[    0  7035     0]\n",
      " [    0 10764     0]\n",
      " [    0  2250     0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "models = {}\n",
    "predictions = []\n",
    "\n",
    "# Train a separate model for each class\n",
    "for target_class in label_mapping.values():\n",
    "    # Create binary labels for the current class\n",
    "    binary_y_train = (y_train == target_class).astype(int)\n",
    "\n",
    "    # print(f\"X_resampled dtype: {X_resampled.dtypes}\")\n",
    "    # print(f\"y_resampled dtype: {y_resampled.dtypes}\")\n",
    "    # print('#################################################')\n",
    "    # print(f\"X_resampled dtype: {X_resampled.dtypes}, shape: {X_resampled.shape}\")\n",
    "    # print(f\"y_resampled dtype: {y_resampled.dtypes}, shape: {y_resampled.shape}\")\n",
    "    # print('#################################################')\n",
    "\n",
    "\n",
    "    # X_train_scaled_np = X_train_scaled.astype(np.float64)\n",
    "    # y_resampled_np = y_resampled.astype(np.float64)\n",
    "    # X_resampled_np = X_resampled.to_numpy(dtype=np.float64)\n",
    "    # y_resampled_np = y_resampled.to_numpy(dtype=np.int64)\n",
    "\n",
    "    # Debug info\n",
    "    # print(f\"Converted X_resampled dtype: {X_resampled_np.dtype}\")\n",
    "    # print(f\"Converted y_resampled dtype: {y_resampled_np.dtype}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Train the SVM model for the current class\n",
    "    model = HardMarginSVM(learning_rate=0.01, n_iter=500)\n",
    "\n",
    "    # Fit the model\n",
    "    print(\"Fitting the model for target class\", target_class)\n",
    "    model.fit(X_train_scaled_np, y_resampled_np)\n",
    "    print(\"Saving the model for target class\", target_class)\n",
    "\n",
    "\n",
    "    # Save the trained model\n",
    "    models[target_class] = model\n",
    "\n",
    "    # Predict scores for the test data\n",
    "    print(\"Predicting the scores for target class\", target_class)\n",
    "    scores = model.decision_function(X_test_scaled)\n",
    "    predictions.append(scores)\n",
    "    print(\"Predicting the scores for target class done\", target_class)\n",
    "\n",
    "# # Combine predictions to determine the final class for each test instance\n",
    "# predictions = np.array(predictions)\n",
    "# final_predictions = np.argmax(predictions, axis=0) # Assign class with the highest score\n",
    "# final_predictions = final_predictions.astype(int)  # Convert to integers if needed\n",
    "# print(\"Final prediction of classes DONE!!!\")\n",
    "# print(f\"y_test type: {type(y_test)}, y_test unique values: {np.unique(y_test)}\")\n",
    "# print(f\"final_predictions type: {type(final_predictions)}, final_predictions unique values: {np.unique(final_predictions)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "# Ensure that predictions shape is (num_classes, num_samples) for np.argmax\n",
    "if predictions.shape[0] == len(label_mapping):\n",
    "    final_predictions = np.argmax(predictions, axis=0)  # Assign class with the highest score\n",
    "else:\n",
    "    # Handle case where predictions do not align with number of target classes\n",
    "    raise ValueError(\"Mismatch between number of target classes and predictions\")\n",
    "\n",
    "final_predictions = final_predictions.astype(int)  # Convert to integers if needed\n",
    "final_predictions = np.array(final_predictions).astype(int)\n",
    "print(\"Final prediction of classes DONE!!!\")\n",
    "print(f\"y_test type: {type(y_test)}, y_test unique values: {np.unique(y_test)}\")\n",
    "print(f\"final_predictions type: {type(final_predictions)}, final_predictions unique values: {np.unique(final_predictions)}\")\n",
    "print('************************************************************************************')\n",
    "print(f\"final_predictions: {final_predictions}\")\n",
    "\n",
    "\n",
    "# print('***************** mapped final predictions', final_predictions)\n",
    "print('************************************************************************************')\n",
    "print(\"y_test unique values and counts:\", y_test.value_counts())\n",
    "print('************************************************************************************')\n",
    "print(\"final_predictions unique values and counts:\", np.unique(final_predictions, return_counts=True))\n",
    "print('************************************************************************************')\n",
    "# Evaluate the model\n",
    "print('Evaluating the model')\n",
    "accuracy = accuracy_score(y_test, final_predictions)\n",
    "print('accuracy', accuracy)\n",
    "precision = precision_score(y_test, final_predictions, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_test, final_predictions, average='weighted', zero_division=0)\n",
    "f1 = f1_score(y_test, final_predictions, average='weighted', zero_division=0)\n",
    "conf_matrix = confusion_matrix(y_test, final_predictions)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZVCqhyUibVw"
   },
   "source": [
    "## Observations:\n",
    "\n",
    "* Based on the results of the confusion matrix and final predictions, its evident that model is not able to classify all catergories properly. The model is being biased towards the category 'NO'.\n",
    "\n",
    "* The accuracy we got is close to 54%, which seems to be in a satisfactory range. But based on the precision value of 0.288 and the output of the confusion matrix, the model is not classifying efficiently. Based on further examination we came to umderstand that all the prediction values of the model are 0 i.e., 'NO' category.\n",
    "\n",
    "### Reasons for such performance:\n",
    "* The dataset itself is biases towards the category 'NO'\n",
    "* Many features have low variance in the dataset, leading to not enough information for the model to learn patterns and able to classify the target variable properly.\n",
    "* Also, training the SVM for less number of epochs, due to lack of computational resources and limited time constraints.\n",
    "\n",
    "### Further improvements for better results:\n",
    "* Gather more data that is un-biased and have columns with high entropy values might help the model to identify and learn more insights.\n",
    "* Leveraging additonal computational resources like powerful GPU's and training the model for huge number of epochs - 1000 or more - might improve the model performance.\n",
    "* Apart from linear kernel usage of other kernals, such as polynomial or rbf kernels, could provide some better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqcNs4iDibVw"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
